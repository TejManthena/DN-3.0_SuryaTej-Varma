Understanding :

Big O Notation

Big O notation is a mathematical notation used to describe the upper bound of an algorithm's running time or space requirements in terms of input size. It provides a high-level understanding of the algorithm's efficiency and scalability. Big O notation helps in comparing different algorithms and understanding their behavior as the input size grows.

Best, Average, and Worst-Case Scenarios

Best Case: The Case where algorithm performs the minimum number of operations. In Case of the search algorithm it means the algorithm can able to find the solution in the very first attempt

Average Case: The Case where the algorithm goes to large number of inputs. It will check through all the possible outputs.

Worst Case: The Case where the algorithm performs the maximum number of operations. This has a high num time, sometimes it also means checking till the last case
